{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0e7abe5-1357-4f85-8ae7-1ee88a0e5fb2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "928e5740-13dd-4565-86f5-24ebf9f59562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6caca23-f8b6-42fa-b652-db1093701cfc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6b33c90-49b7-4466-8688-454d5a90698a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2vec is a shallow, two-layer neural network technique developed by Google that converts text into numerical vectors (word embeddings) to capture semantic relationships and meanings. It analyzes large text corpora to represent words in a continuous vector space where similar words are closer toge\n"
     ]
    }
   ],
   "source": [
    "with open(\"Text.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665fb5a5-3deb-4c2c-97c7-8c66d59641cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0d45682-4486-443e-a7b4-e7a560621641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing (Tokenization, Vocabulary, Encoding)\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", text)\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def build_vocab(tokens, min_freq=1, max_vocab_size=None):\n",
    "    counter = Counter(tokens)\n",
    "    items = [(w, c) for w, c in counter.items() if c >= min_freq]\n",
    "    items.sort(key=lambda x: -x[1])\n",
    "\n",
    "    if max_vocab_size is not None:\n",
    "        items = items[:max_vocab_size]\n",
    "\n",
    "    word2id = {w: i for i, (w, _) in enumerate(items)}\n",
    "    id2word = {i: w for w, i in word2id.items()}\n",
    "    return word2id, id2word\n",
    "\n",
    "def encode(tokens, word2id):\n",
    "    return [word2id[w] for w in tokens if w in word2id]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5b165da-9158-40cf-9aa0-99bacce20a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 104\n",
      "First 20 tokens: ['word2vec', 'is', 'a', 'shallow', 'two', 'layer', 'neural', 'network', 'technique', 'developed', 'by', 'google', 'that', 'converts', 'text', 'into', 'numerical', 'vectors', 'word', 'embeddings']\n",
      "First 20 encoded: [4, 5, 1, 31, 32, 33, 34, 35, 36, 37, 38, 39, 12, 13, 6, 14, 40, 15, 2, 41]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenize(text)\n",
    "word2id, id2word = build_vocab(tokens)\n",
    "encoded = encode(tokens, word2id)\n",
    "\n",
    "print(\"Vocab size:\", len(word2id))\n",
    "print(\"First 20 tokens:\", tokens[:20])\n",
    "print(\"First 20 encoded:\", encoded[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dab376b-a8ac-4e41-8225-a7aa96039642",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generate Skip-Gram pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7dc1c29-1bb6-494f-8703-c463cfd02f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_skipgram_pairs(encoded_tokens, window_size=2):\n",
    "    pairs = []\n",
    "    for i, center in enumerate(encoded_tokens):\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(encoded_tokens), i + window_size + 1)\n",
    "        for j in range(start, end):\n",
    "            if i != j:\n",
    "                context = encoded_tokens[j]\n",
    "                pairs.append((center, context))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efd8abf0-78a1-4f32-b7b7-e7cfa4a3b742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training pairs: 614\n",
      "First 10 pairs: [(4, 5), (4, 1), (5, 4), (5, 1), (5, 31), (1, 4), (1, 5), (1, 31), (1, 32), (31, 5)]\n"
     ]
    }
   ],
   "source": [
    "pairs = generate_skipgram_pairs(encoded, window_size=2)\n",
    "\n",
    "print(\"Number of training pairs:\", len(pairs))\n",
    "print(\"First 10 pairs:\", pairs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fec7709-5614-4895-baa1-d20674211ddc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ea469c7-2a12-41f0-be88-cb495d589a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_negative_words(vocab_size, positive_word, K=5):\n",
    "    neg_samples = []\n",
    "    while len(neg_samples) < K:\n",
    "        neg = np.random.randint(0, vocab_size)\n",
    "        if neg != positive_word:\n",
    "            neg_samples.append(neg)\n",
    "    return neg_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8931e7ca-d6a9-4ba4-af38-a70f2c8137e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example negative samples: [75, 1, 26, 63, 79]\n"
     ]
    }
   ],
   "source": [
    "print(\"Example negative samples:\", sample_negative_words(len(word2id), positive_word=0, K=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6418416-8db0-4b41-b71f-9591cff21d32",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Word2Vec Model (Numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "185fe0f1-d247-4ec7-bb78-77f2da01f216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "class Word2Vec:\n",
    "    def __init__(self, vocab_size, embedding_dim=50, learning_rate=0.01):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        # Input embeddings (center words)\n",
    "        self.W_in = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "\n",
    "        # Output embeddings (context words)\n",
    "        self.W_out = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "\n",
    "    def forward(self, center_id, context_id, neg_ids):\n",
    "        v_c = self.W_in[center_id]           # center word vector\n",
    "        u_o = self.W_out[context_id]         # positive context vector\n",
    "        u_neg = self.W_out[neg_ids]          # negative samples vectors\n",
    "\n",
    "        # Positive score\n",
    "        pos_score = np.dot(v_c, u_o)\n",
    "        pos_prob = sigmoid(pos_score)\n",
    "\n",
    "        # Negative scores\n",
    "        neg_scores = np.dot(u_neg, v_c)\n",
    "        neg_prob = sigmoid(-neg_scores)\n",
    "\n",
    "        # Loss = -log(sigmoid(v_c · u_o)) - sum(log(sigmoid(-v_c · u_neg)))\n",
    "        loss = -np.log(pos_prob) - np.sum(np.log(neg_prob))\n",
    "\n",
    "        return pos_prob, neg_prob, loss, v_c, u_o, u_neg\n",
    "\n",
    "    def backward(self, center_id, context_id, neg_ids, pos_prob, neg_prob, v_c, u_o, u_neg):\n",
    "        # Gradient for center word vector\n",
    "        grad_v = (pos_prob - 1) * u_o + np.sum(neg_prob[:, None] * u_neg, axis=0)\n",
    "\n",
    "        # Gradient for positive context word\n",
    "        grad_u_o = (pos_prob - 1) * v_c\n",
    "\n",
    "        # Gradient for negative samples\n",
    "        grad_u_neg = neg_prob[:, None] * v_c\n",
    "\n",
    "        # Update center word embedding\n",
    "        self.W_in[center_id] -= self.lr * grad_v\n",
    "\n",
    "        # Update positive context embedding\n",
    "        self.W_out[context_id] -= self.lr * grad_u_o\n",
    "\n",
    "        # Update negative sample embeddings\n",
    "        for i, neg_id in enumerate(neg_ids):\n",
    "            self.W_out[neg_id] -= self.lr * grad_u_neg[i]\n",
    "\n",
    "    def train_step(self, center_id, context_id, neg_ids):\n",
    "        pos_prob, neg_prob, loss, v_c, u_o, u_neg = self.forward(center_id, context_id, neg_ids)\n",
    "        self.backward(center_id, context_id, neg_ids, pos_prob, neg_prob, v_c, u_o, u_neg)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f317ff44-dfdf-41cc-bcce-d57dbc2dc995",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ebca6d2-7e0b-40df-a34e-fc9847c47432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Average Loss: 4.1588\n",
      "Epoch 2/5 - Average Loss: 4.1588\n",
      "Epoch 3/5 - Average Loss: 4.1587\n",
      "Epoch 4/5 - Average Loss: 4.1586\n",
      "Epoch 5/5 - Average Loss: 4.1585\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(\n",
    "    vocab_size=len(word2id),\n",
    "    embedding_dim=50,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "epochs = 5\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for center, context in pairs:\n",
    "        neg_ids = sample_negative_words(len(word2id), context, K=5)\n",
    "        loss = model.train_step(center, context, neg_ids)\n",
    "        total_loss += loss\n",
    "\n",
    "    avg_loss = total_loss / len(pairs)\n",
    "    loss_history.append(avg_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Average Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e240034-b1f7-4941-bb9d-c0d6abac3a94",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Similarity Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e824e407-8232-4f03-91cd-6c5d52457706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def get_vector(word):\n",
    "    return model.W_in[word2id[word]]\n",
    "\n",
    "def most_similar(word, top_n=5):\n",
    "    if word not in word2id:\n",
    "        raise ValueError(f\"'{word}' is not in the vocabulary.\")\n",
    "\n",
    "    target_vec = get_vector(word)\n",
    "    similarities = []\n",
    "\n",
    "    for w, idx in word2id.items():\n",
    "        if w == word:\n",
    "            continue\n",
    "        sim = cosine_similarity(target_vec, model.W_in[idx])\n",
    "        similarities.append((w, sim))\n",
    "\n",
    "    similarities.sort(key=lambda x: -x[1])\n",
    "    return similarities[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "513bcbf7-a53d-47f7-bd88-4150526895d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['words',\n",
       " 'a',\n",
       " 'word',\n",
       " 'to',\n",
       " 'word2vec',\n",
       " 'is',\n",
       " 'text',\n",
       " 'and',\n",
       " 'it',\n",
       " 'in',\n",
       " 'similar',\n",
       " 'on',\n",
       " 'that',\n",
       " 'converts',\n",
       " 'into',\n",
       " 'vectors',\n",
       " 'semantic',\n",
       " 'relationships',\n",
       " 'meanings',\n",
       " 'continuous',\n",
       " 'used',\n",
       " 'sentiment',\n",
       " 'analysis',\n",
       " 'search',\n",
       " 'relevance',\n",
       " 'of',\n",
       " 'predicts',\n",
       " 'target',\n",
       " 'based',\n",
       " 'surrounding',\n",
       " 'context',\n",
       " 'shallow',\n",
       " 'two',\n",
       " 'layer',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'technique',\n",
       " 'developed',\n",
       " 'by',\n",
       " 'google',\n",
       " 'numerical',\n",
       " 'embeddings',\n",
       " 'capture',\n",
       " 'analyzes',\n",
       " 'large',\n",
       " 'corpora',\n",
       " 'represent',\n",
       " 'vector',\n",
       " 'space',\n",
       " 'where',\n",
       " 'are',\n",
       " 'closer',\n",
       " 'together',\n",
       " 'primarily',\n",
       " 'for',\n",
       " 'nlp',\n",
       " 'tasks',\n",
       " 'like',\n",
       " 'machine',\n",
       " 'translation',\n",
       " 'key',\n",
       " 'aspects',\n",
       " 'purpose',\n",
       " 'unstructured',\n",
       " 'high',\n",
       " 'dimensional',\n",
       " 'enabling',\n",
       " 'mathematical',\n",
       " 'operations',\n",
       " 'e',\n",
       " 'g',\n",
       " 'king',\n",
       " 'man',\n",
       " 'woman',\n",
       " 'queen',\n",
       " 'core',\n",
       " 'principle',\n",
       " 'appear',\n",
       " 'contexts',\n",
       " 'tend',\n",
       " 'have',\n",
       " 'architectures',\n",
       " 'bag',\n",
       " 'cbow',\n",
       " 'skip',\n",
       " 'gram',\n",
       " 'single',\n",
       " 'applications',\n",
       " 'content',\n",
       " 'recommendation',\n",
       " 'similarity',\n",
       " 'detection',\n",
       " 'crucial',\n",
       " 'because',\n",
       " 'allows',\n",
       " 'computer',\n",
       " 'models',\n",
       " 'understand',\n",
       " 'the',\n",
       " 'rather']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word2id.keys())[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2746bd36-9db2-400b-8c28-ce11ecd2acf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('embeddings', np.float64(0.3337463300789198)),\n",
       " ('developed', np.float64(0.2822875520707103)),\n",
       " ('because', np.float64(0.26087842740835915)),\n",
       " ('semantic', np.float64(0.2554218900462723)),\n",
       " ('and', np.float64(0.2547947462950354))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"understand\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
